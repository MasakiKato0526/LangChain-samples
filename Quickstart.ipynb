{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Guide \n",
    "この Notebook は、LangChain の Quickstart Guide を以下の条件で書き換えたものになります。\n",
    "- Azure OpenAI Service を使用\n",
    "- モデルは\n",
    "    - text-davinchi-003 (デプロイ名：text-davinchi-003)\n",
    "    - gpt-4 (デプロイ名：gpt-4)\n",
    "\n",
    "以下のリンクは、オリジナルのサイトになります。こちらも参照するようにしてください。  \n",
    "https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enironment Setup\n",
    "必要なパラメータの値を、.env ファイルより取得します。あらかじめ.env ファイルに必要な値を設定しておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# .env ファイルから環境変数を読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# モデルのデプロイ名をパラメータに設定\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\")    # text-davinchi-003 のデプロイ名\n",
    "AZURE_OPENAI_GPT4_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT4_DEPLOYMENT\")  # gpt-4 のデプロイ名\n",
    "\n",
    "# OpenAI API が使用するパラメータの設定\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")                            # Azure OpenAI API エンドポイント\n",
    "openai.api_version = os.environ.get(\"OPENAI_API_VERSION\")                      # Azure OpenAI API バージョン\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")                              # Azure OpenAI API キー"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Language Model Application: LLMs\n",
    "LangChainには、言語モデルアプリケーションを構築するために使用できる多数のモジュールがあります。モジュールを組み合わせてより複雑なアプリケーションを作成することもでき、単純なアプリケーションに個別に使用することもできます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs: Get predictions from a language \n",
    "LangChain の最も基本的な構成要素は、入力に対して LLM を呼び出すことです。これを行う簡単な例を紹介しましょう。ここでは企業が製造している商品に基づいて会社名を生成するサービスを構築するとします。  \n",
    "これを行うためには、まず LLM ラッパーをインポートする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import AzureOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その後、任意の引数でラッパーを初期化することができます。この例では、text-davinchi-003 モデルを使用し、出力をよりランダムにするための高めの temperature で初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=AZURE_OPENAI_GPT_DEPLOYMENT, temperature=0.9, openai_api_key=openai.api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、何かしらの入力に対してLLMを呼び出してみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates: Manage prompts for LLMs\n",
    "LLM を呼び出すことは素晴らしい最初のステップになりますが、それは始まりにすぎません。通常、アプリケーションで LLM を使用する場合、ユーザーの入力を直接 LLM に送信するわけではありません。代わりに、ユーザーの入力を取得し、プロンプトを構築してから LLMに送信することが多いでしょう。  \n",
    "例えば前の例では、私たちが渡したテキストはカラフルな靴下を製造する会社の名前を求めるようにハードコードされていました。この想像上のサービスでは、ユーザーが会社が何をするかを説明する入力のみを受け取り、その情報を持ったプロンプトをフォーマットしたいと思うでしょう。  \n",
    "これはLangChainで簡単に行えます！  \n",
    "まず、プロンプトのテンプレートを定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What would be good company name for a company that makes {product}?\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、これがどのように機能するか見てみましょう！ `.format` メソッドを呼び出して、プロンプトをフォーマットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains: Combine LLMs and prompts in multi-step workflows\n",
    "ここまで、PromptTemplate や LLM といったプリミティブを単独で扱ってきましたが、実際のアプリケーションは単一のプリミティブではなく、それらを組み合わせたものです。\n",
    "LangChain のチェーンは、LLM や他のチェーンなどのプリミティブとなるリンクで構成されています。\n",
    "最も基本的なチェーンのタイプは、PromptTemplate と LLM から構成される LLMChain です。\n",
    "前の例を拡張して、ユーザーの入力を取得し、PromptTemplate でフォーマットし、フォーマットされた応答を LLM に渡す LLMChain を構築します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、製品を指定するだけのチェーンを実行します！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"color socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やりましたね！ これが最初のチェーン、LLM Chain です。これはよりシンプルなチェーンのタイプの1つですが、その動作原理を理解することは、より複雑なチェーンを扱うための良いスタートになります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents: Dynamically Call Chains Based on User Input\n",
    "今まで見てきたチェーンは、あらかじめ決められた順序で実行されていました。\n",
    "\n",
    "しかし、エージェントは異なります。エージェントは LLM を使用して、どのアクションを実行するか、そしてどの順序で実行するかを決定します。アクションは、ツールを使用して出力を監視する、もしくはユーザーに返すことができます。\n",
    "\n",
    "エージェントを正しく使用すると、非常に強力なものになります。このチュートリアルでは、最もシンプルでハイレベルの API を通じて、エージェントを簡単に使用する方法を紹介します。\n",
    "\n",
    "エージェントをロードするには、以下の概念を理解する必要があります。\n",
    "- ツール: 特定の役割を果たす関数です。これには、Google 検索、データベースの検索、Python REPL、他のチェーンなどが含まれます。ツールのインターフェイスは、現在、文字列を入力として受け取り、文字列を出力として返す関数である必要があります。\n",
    "- LLM: エージェントの駆動力となる言語モデル。\n",
    "- エージェント: 使用するエージェントです。これは、サポートされているエージェントクラスを参照する文字列である必要があります。このノートブックでは、最もシンプルでハイレベルのAPIに焦点を当てているため、標準のサポートされているエージェントのみを使用する方法について説明しています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは SerpAPI を使用します。SerpAPI は、サーチエンジンをスクレイプする API で、以下のサイトでサインアップして API Key を取得する必要があります。  \n",
    "https://serpapi.com/  \n",
    "\n",
    "API Key の値を環境変数として設定する必要がありますが、これは.env ファイルからすでに読み込み済みです。  \n",
    "\n",
    "それでは始めてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# text-davinchi-003 モデルをロード\n",
    "# ここでは Temperature を最も低い値に設定\n",
    "llm = AzureOpenAI(deployment_name=AZURE_OPENAI_GPT_DEPLOYMENT, temperature=0, openai_api_key=openai.api_key)\n",
    "\n",
    "# 使用するツールをロード\n",
    "# llm-math は数値計算のために使用\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# 使用するツール、言語モデル、そして使用するエージェントの種類を指定して、エージェントを初期化\n",
    "agent = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# エージェントの実行\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory: Add State to Chains and Agents\n",
    "これまでに私たちが経験してきたすべてのチェーンやエージェントはステートレスでした。しかし、時にはチェーンやエージェントが前回のやりとりに関する情報を記憶できるようにするために、「メモリ」の概念を持つことが望ましい場合があります。最も明確で単純な例は、チャットボットを設計するときです。チャットボットが前のメッセージを記憶して、それを利用してより良い会話を行えるようにすることが望まれます。これは「短期記憶」の一種です。より複雑な例では、チェーン/エージェントが時間の経過とともに重要な情報を記憶することができるようにすることができます。これは「長期記憶」の一形態になります。\n",
    "\n",
    "LangChain には、この目的のために特別に作成されたいくつかのチェーンがあります。このノートブックでは、このようなチェーンの1つである、2種類のメモリを持つ`ConversationChain` を使用する方法について説明します。\n",
    "\n",
    "デフォルトでは、`ConversationChain` は、以前のすべての入力/出力を記憶してコンテキストに追加する単純なタイプのメモリを持ちます。このチェーンを使用する方法を見てみましょう（プロンプトを表示するために `verbose=True` を設定します）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conversation.predict(input=\"I'm doing well!  Just having a conversation with an AI.\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Language Model Application: Chat Models\n",
    "同様に、LLM の代わりにチャットモデルを使用することができます。チャットモデルは、言語モデルの1つのバリエーションです。チャットモデルは内部で言語モデルを使用している一方で、公開されているインターフェースは少し異なります。 「テキストを入力し、テキストを出力する」API ではなく、「チャットメッセージ」が入力と出力の両方になるインターフェースを公開しています。  \n",
    "チャットモデル API は比較的新しいため、正しい抽象化方法をまだ見つけている途中です。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Message Completions from a Chat Model\n",
    "チャットモデルに1つまたは複数のメッセージを渡すことで、チャットの Completion を取得できます。応答はメッセージになります。現在、LangChain でサポートされているメッセージの種類には、`AIMessage`、`HumanMessage`、`SystemMessage`、および`ChatMessage` があります。`ChatMessage` は任意の役割パラメータを受け取ります。ほとんどの場合、`HumanMessage`、`AIMessage`、および `SystemMessage` を扱うことになるでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import (\n",
    "    HumanMessage, \n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(deployment_name=AZURE_OPENAI_GPT4_DEPLOYMENT, temperature=0, openai_api_key=openai.api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一のメッセージを渡すことで、Completion を取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to Japanese. I love programming.\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI の gpt-3.5-turbo と gpt-4 モデルでは、複数のメッセージを渡すこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [SystemMessage(content=\"You are a helpful assistant that translate English to Japanese.\"),\n",
    "           HumanMessage(content=\"Translate this sentence from English to Japanese. I love programming.\")]\n",
    "\n",
    "chat(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、`generate` を使用して複数のメッセージセットに対して Completion を生成することもできます。この場合は、追加の `message` パラメータを持つ`LLMResult` が返されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Japanese. I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Japanese.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to Japanese. I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_messages)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この LLMResult からトークンの使用状況などを取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output['token_usage']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Prompt Templates\n",
    "LLM と同様に、`MessagePromptTemplate` を使用したテンプレートを利用することができます。`MessagePromptTemplate` から `ChatPromptTemplate` を構築することができ、`ChatPromptTemplate` では `format_prompt` を使用することができます。これは `PromptValue` を返しますが、フォーマットされた値を LLM またはチャットモデルの入力として使用するかどうかに応じて、文字列またはメッセージオブジェクトに変換することができます。\n",
    "\n",
    "テンプレートに対して使用できる `from_template` メソッドがあります。このテンプレートを使用すると、以下のように記述することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Japanese\", \n",
    "                               text=\"I love programming.\").to_messages())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains with Chat Models\n",
    "前述のセクションで説明した `LLMChain` は、チャットモデルでも使用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You ara a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Japanese\", text=\"I love programming.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents with Chat Models\n",
    "エージェントは、チャットモデルでも使用できます。エージェントを初期化する場合は、エージェントタイプとして「zero-shot-react-description」を使用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するツールをロード\n",
    "# llm-math は数値計算のために使用\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=chat)\n",
    "\n",
    "# 使用するツール、言語モデル、そして使用するエージェントの種類を指定して、エージェントを初期化\n",
    "agent = initialize_agent(tools=tools, llm=chat, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# エージェントの実行\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory: Add State to Chains and Agents\n",
    "チャットモデルを使用して初期化されたチェーンとエージェントで Memory を使用することができます。LLM の Memory との主な違いは、過去のすべてのメッセージを文字列にまとめるのではなく、独自のユニークなメモリオブジェクトとして保持できることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=AZURE_OPENAI_GPT4_DEPLOYMENT, temperature=0, openai_api_key=openai.api_key)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm) \n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "\n",
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory に格納されたメッセージを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求プロンプトに入力できるトークン数には制限があるため、必要に応じて格納されたメッセージを消去します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
